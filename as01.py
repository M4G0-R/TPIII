# -*- coding: utf-8 -*-
"""AS01

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LYd50ODVblhTidfIivMQlxXCMnjm1bgQ

# Dados do Aluno

- Aluno: Pedro Igor Martins dos Reis
- Curso: Engenharia de Computação
- Matrícula: 648166

# Normalização
"""

# Importando as bibliotecas necessárias
import gdown
import re
import unicodedata

# Função de normalização
def normalize_text(text):
    text = text.lower()
    text = ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')
    text = re.sub(r'[^\w\s\.\$]', '', text)
    return text

# Carregar e ler o arquivo
url = 'https://raw.githubusercontent.com/M4G0-R/TPIII/main/Shakespeare.txt'
output = '/content/Shakespeare.txt'
gdown.download(url, output, quiet=True, fuzzy=True)
with open('/content/Shakespeare.txt', 'r', encoding='utf-8') as file:
    content = file.read()

# Normalizar o texto
normalized_content = normalize_text(content)

# Salvar o texto normalizado em um novo arquivo
with open('/content/Shakespeare_Normalized.txt', 'w', encoding='utf-8') as normalized_file:
    normalized_file.write(normalized_content)

print("A normalização do texto foi concluída e salva em 'Shakespeare_Normalized.txt'")

"""# Tokenização"""

import nltk
from textblob import TextBlob
import spacy
from gensim.utils import tokenize
from keras.preprocessing.text import text_to_word_sequence
import re

# Preparação
nltk.download('punkt')
spacy.cli.download("en_core_web_sm")
nlp = spacy.load("en_core_web_sm")

# Função para ler o arquivo normalizado
def read_normalized_file(filepath):
    with open(filepath, 'r', encoding='utf-8') as file:
        return file.read()

# Funções de Tokenização
def nltk_mwe_tokenizer(text, mwes=[('ice', 'cream'), ('U', 'S', 'market')]):
    tokenizer = nltk.MWETokenizer(mwes)
    return tokenizer.tokenize(text.split())

# Adaptação para ler o arquivo uma única vez
normalized_text = read_normalized_file('/content/Shakespeare_Normalized.txt')

# Lista de funções de tokenização e seus respectivos processamentos
tokenization_functions = [
    ('White Space Tokenization', lambda text: text.split()),
    ('NLTK Word Tokenizer', nltk.word_tokenize),
    ('NLTK Treebank Word Tokenizer', nltk.TreebankWordTokenizer().tokenize),
    ('NLTK WordPunct Tokenizer', nltk.WordPunctTokenizer().tokenize),
    ('NLTK Tweet Tokenizer', nltk.TweetTokenizer().tokenize),
    ('NLTK MWE Tokenizer', nltk_mwe_tokenizer),
    ('TextBlob Word Tokenizer', lambda text: list(TextBlob(text).words)),
    ('spaCy Tokenizer', lambda text: [token.text for token in nlp(text)]),
    ('Gensim Word Tokenizer', lambda text: list(tokenize(text))),
    ('Keras Tokenizer', lambda text: text_to_word_sequence(text))
]

# Execução e armazenamento dos resultados de tokenização
for i, (name, func) in enumerate(tokenization_functions, start=1):
    tokens = func(normalized_text)
    with open(f'/content/Shakespeare_Normalized_Tokenized{i:02}.txt', 'w', encoding='utf-8') as file:
        for token in tokens:
            file.write(f"{token}\n")
    print(f"Função {name} finalizada e arquivo salvo em /content/Shakespeare_Normalized_Tokenized{i:02}.txt")

"""# Stop-words Removal"""

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('punkt')
nltk.download('stopwords')

# Ler o arquivo tokenizado da subtarefa 2
with open('/content/Shakespeare_Normalized_Tokenized02.txt', 'r', encoding='utf-8') as file:
    tokenized_text = file.read()

# Converter o texto em tokens
tokens = word_tokenize(tokenized_text)

# Remover stop-words
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word.lower() not in stop_words]

# Salvar os tokens filtrados em um novo arquivo
with open('/content/Shakespeare_Normalized_Tokenized_StopWord.txt', 'w', encoding='utf-8') as output_file:
    for token in filtered_tokens:
        output_file.write(f"{token}\n")

print("Stop-words removal finalizado e salvo em /content/Shakespeare_Normalized_Tokenized_StopWord.txt")

"""# Text Lemmatization"""

import nltk
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Função para mapear a tag do POS do NLTK para o formato aceito pelo WordNetLemmatizer
def get_wordnet_pos(word):
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)

# Inicializar o lematizador
lemmatizer = WordNetLemmatizer()

# Ler o arquivo de entrada
with open('/content/Shakespeare_Normalized_Tokenized_StopWord.txt', 'r', encoding='utf-8') as file:
    tokenized_text = file.read()

# Tokenizar o texto
tokens = word_tokenize(tokenized_text)

# Lematização
lemmatized_tokens = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in tokens]

# Salvar os tokens lematizados em um novo arquivo
with open('/content/Shakespeare_Normalized_Tokenized_StopWord_Lemmatized.txt', 'w', encoding='utf-8') as output_file:
    for token in lemmatized_tokens:
        output_file.write(f"{token}\n")

print("Lematização finalizada e arquivo salvo em /content/Shakespeare_Normalized_Tokenized_StopWord_Lemmatized.txt")

"""# Text Stemming"""

import nltk
from nltk.stem import PorterStemmer, SnowballStemmer
from nltk.tokenize import word_tokenize

nltk.download('punkt')

# Inicializar os stemmers
porter = PorterStemmer()
snowball = SnowballStemmer(language='english')

# Ler o arquivo de entrada
with open('/content/Shakespeare_Normalized_Tokenized_StopWord_Lemmatized.txt', 'r', encoding='utf-8') as file:
    lemmatized_text = file.read()

# Tokenizar o texto
tokens = word_tokenize(lemmatized_text)

# Aplicar Porter Stemmer
porter_stemmed_tokens = [porter.stem(token) for token in tokens]
# Salvar os tokens stemizados pelo Porter Stemmer
with open('/content/Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming01.txt', 'w', encoding='utf-8') as output_file:
    for token in porter_stemmed_tokens:
        output_file.write(f"{token}\n")

# Aplicar Snowball Stemmer
snowball_stemmed_tokens = [snowball.stem(token) for token in tokens]
# Salvar os tokens stemizados pelo Snowball Stemmer
with open('/content/Shakespeare_Normalized_Tokenized_StopWord_Lemmatized_Stemming02.txt', 'w', encoding='utf-8') as output_file:
    for token in snowball_stemmed_tokens:
        output_file.write(f"{token}\n")

print("Stemming finalizado e salvo nos respectivos arquivos.")

"""# Análise do Vocabulário"""

import pandas as pd
from collections import Counter
from nltk.tokenize import word_tokenize

# Exemplo de como ler e processar o arquivo lematizado
with open('/content/Shakespeare_Normalized_Tokenized_StopWord_Lemmatized.txt', 'r', encoding='utf-8') as file:
    text = file.read()

# Tokenização do texto lematizado
tokens = word_tokenize(text)

# Contagem de ocorrências de cada token
token_counts = Counter(tokens)

# Preparar dados para o CSV
data = {'Token': list(token_counts.keys()),
        'Número de Ocorrências': list(token_counts.values()),
        'Tamanho em Caracteres': [len(token) for token in token_counts.keys()]}

# Converter para DataFrame e salvar em CSV
df = pd.DataFrame(data)
df.to_csv('/content/Shakespeare_Vocabulary_Lemmatized.csv', index=False)

print("Arquivo CSV para lematização gerado com sucesso.")
